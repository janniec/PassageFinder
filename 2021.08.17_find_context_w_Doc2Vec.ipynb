{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aeaf13a",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b9ae165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import multiprocessing\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87f18e5",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68eb5489",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('dev-v2.0.json')\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66d3b806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiles contexts & build a guide to trace context back to paragraphs & questions\n",
    "contexts = []\n",
    "contexts_to_data = {}\n",
    "questions=[]\n",
    "questions_to_data = {}\n",
    "context_new_i = 0\n",
    "question_new_i = 0\n",
    "for p_i, paragraph_sets in enumerate(data['data']):\n",
    "    for c_i, context_sets in enumerate(data['data'][p_i]['paragraphs']):\n",
    "        contexts.append(data['data'][p_i]['paragraphs'][c_i]['context'])\n",
    "        contexts_to_data[context_new_i] = (p_i, c_i)\n",
    "        for q_i, question_sets in enumerate(data['data'][p_i]['paragraphs'][c_i]['qas']):\n",
    "            questions.append(data['data'][p_i]['paragraphs'][c_i]['qas'][q_i]['question'])\n",
    "            if data['data'][p_i]['paragraphs'][c_i]['qas'][q_i]['is_impossible'] == False:\n",
    "                questions_to_data[question_new_i] = (context_new_i, p_i, c_i)\n",
    "            else:\n",
    "                questions_to_data[question_new_i] = ('impossible', p_i, c_i)\n",
    "            question_new_i +=1\n",
    "        context_new_i +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d3d2b0",
   "metadata": {},
   "source": [
    "# Train with Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd9b90",
   "metadata": {},
   "source": [
    "Why am I using Doc2Vec for document similiarity?   \n",
    "The newer techniques such as BERT can only take in a maximum of 512 tokens\n",
    "for this step, i wanted a model that could handle any length of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d099ef",
   "metadata": {},
   "source": [
    "Parameter research: dm vs dbow  \n",
    "dm (distributed memory): randomly samples consecutive words from a paragraph, randomly samples sets of words, using those words as input, predicts a center word. (kind of like CBOW, continuous bag of words) Doc vectors and word vectors are average together. co trains word vectors\n",
    "dbow (distributed bag of words): ignores context words & has to predict words randomly sampled from the paragraph. \n",
    "dbow_words = 1, skip-gram word vector will also be trained (in addition to doc-vectors over the whole text), word-vectors over each sliding context window will be trained.\n",
    "  \n",
    "If you only need doc-vectors, use Doc2Vec in a mode that doesn't create or word-vectors (pure PV-DBOW, dm=0, dbow_words=0) or a Doc2Vec mode that also happens to create word-vectors but just choose to ignore them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca3fbdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(contexts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98541eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model = Doc2Vec(dm=0, dbow_words=0, vector_size=200, min_count=0, seed=42, workers=cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0002c1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 10s, sys: 3.73 s, total: 1min 14s\n",
      "Wall time: 42.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "model.build_vocab(documents)\n",
    "model.train(documents, total_examples=model.corpus_count, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d95975bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Doc2Vec_squad2.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23525ac5",
   "metadata": {},
   "source": [
    "# Pick a question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e0bb27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are other major fatality causes?\n",
      "594\n"
     ]
    }
   ],
   "source": [
    "q_i = 6000\n",
    "question = questions[q_i]\n",
    "print(question)\n",
    "context_new_i, p_i, c_i = questions_to_data[q_i]\n",
    "print(context_new_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2faf6643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construction is one of the most dangerous occupations in the world, incurring more occupational fatalities than any other sector in both the United States and in the European Union. In 2009, the fatal occupational injury rate among construction workers in the United States was nearly three times that for all workers. Falls are one of the most common causes of fatal and non-fatal injuries among construction workers. Proper safety equipment such as harnesses and guardrails and procedures such as securing ladders and inspecting scaffolding can curtail the risk of occupational injuries in the construction industry. Other major causes of fatalities in the construction industry include electrocution, transportation accidents, and trench cave-ins.\n"
     ]
    }
   ],
   "source": [
    "context = contexts[context_new_i]\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f9832d",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27a0d2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nlp(question)\n",
    "tokens = [token.lemma_ for token in text]\n",
    "model.random.seed(42)\n",
    "vector = model.infer_vector(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee96d61",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62421e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [(1- cdist(vector.reshape(1,-1)\n",
    "                    , model.dv[i].reshape(1,-1)\n",
    "                    , 'cosine'))[0][0] \n",
    "          for i in range(len(model.dv))]\n",
    "ranks = np.argsort(scores)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a0be1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top contexts:  [ 672  558   49 1028  553]\n",
      "Top contexts scores:  [0.5827660880269205, 0.5549569234538123, 0.5488332389591334, 0.5390468547532865, 0.5276334579594801]\n"
     ]
    }
   ],
   "source": [
    "print('Top contexts: ', ranks[:5])\n",
    "print('Top contexts scores: ', [scores[i] for i in ranks[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d89c8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right answers score:  0.1865200198304846\n",
      "Right answers rank:  877\n"
     ]
    }
   ],
   "source": [
    "print('Right answers score: ', scores[context_new_i])\n",
    "print('Right answers rank: ', list(ranks).index(context_new_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684df054",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "Shuffle NLP processing & different parameters. Calculate sum of errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
